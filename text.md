*PostgreSQL Observability in High-Throughput Systems*

This document describes a technical architecture for a PostgreSQL observability system designed for high-throughput environments. The system is designed as a comprehensive Grafana dashboard to support performance troubleshooting and relies on customized retrieval and transformation of commonly available Postgres statistics. The system targets two use cases:
1. Real-time database performance monitoring and troubleshooting
2. Post-incident investigations and root-cause analysis

The examples and screenshots in this document draw from a real-world implementation in a Tier-1 telecom operator, where PostgreSQL v.16 database uses more than 50 TB storage and processes up to 100 million transactions per hour.

## Solution Overview ##
The monitoring system uses custom views based on PostgreSQL’s statistics to collect performance metrics. Prometheus scrapes these metrics at one-minute intervals, and Grafana dashboards provide visualization. Each custom view targets a specific area: queries, tables, users, transactions, locks, etc. To reduce noise, the system carefully filters and aggregates data instead of capturing every available statistic.

The solution does not use the default postgres_exporter metrics. All metrics are defined in custom views and exposed through sql_exporter.

The following sections describe each performance area: first the description of the custom view that gathers the data, then how the collected data is visualized, with examples from production dashboards.

## Observing Query Performance ##

Identifying high-impact SQL queries is the first step in performance analysis. PostgreSQL’s pg_stat_statements provides execution counts, timing, row counts, and buffer usage. The monitoring view extends it with pg_database and pg_roles for database and user context, and with pg_stat_kcache to add CPU time and I/O metsdfrics.

In high-throughput environments, pg_stat_statements may record thousands of queries, making full export impractical. To focus on the most relevant workload, the system filters to a “Top Queries” set by ranking statements along several resource dimensions—different ways a query can consume database or system resources. From each dimension, the top 100 queries are selected, and the union of these lists forms the final set.

The dimensions are chosen to reflect common performance bottlenecks:
* Total Execution Time (total_exec_time): cumulative runtime across all executions, showing queries that dominate database time.
* CPU Time (pg_stat_kcache): total CPU usage, highlighting CPU-bound queries.
* I/O Bytes (pg_stat_kcache): total read/write volume, identifying queries stressing storage.
* Execution Count (calls): frequency of execution, surfacing “chatty” queries with high aggregate cost.
* Mean Execution Time (mean_exec_time): average per-execution latency, exposing individually slow queries even if rare.

By combining the top 100 from each dimension, the system captures queries that stand out in any major resource category while limiting the final set to a manageable few hundred for efficient monitoring.

In the reference implementation, statements are identified by their normalized SQL text (parameterized/fingerprinted), not by a numeric query ID. Because many queries only differ deep in the text—often after 500–1,000 characters—the system stores up to 4000 characters of each original SQL string (Grafana limit is 4096)  to retain distinguishing detail. Queries longer than this are truncated. These parameters reflect the observed workload in this deployment and can be adjusted for other environments.

### Visualization of queries ###

Performance troubleshooting, especially done post-factum, frequently requires gaining insight into query behavior over time. The monitoring system provides two complementary views in Grafana: time-series charts for trends and patterns, and summary tables for aggregate statistics over selected intervals.
#### Time-Series Charts for Query Trends ####
Once the top queries are identified and exported to Prometheus, Grafana dashboards present their metrics in time-series charts. These visualizations help engineers understand workload patterns and detect anomalies over time, addressing questions like “When did this query start misbehaving?” or “Is our batch job slowing down?”.

• Executions Per Second: For each top query, one chart plots how many times per second it executes (throughput over time). This reveals the system’s operational rhythm and load profile. For example, you might see a batch job’s query execute at a steady high rate during nightly processing and then drop off. If a normally regular execution rate starts declining over time, that could indicate performance degradation – perhaps the query is slowing down, causing fewer executions to complete per second.

Resource Usage: These charts focus on resource consumption over time, broken down by query. For example, a stacked area chart for CPU time per second shows how much total CPU is being used by PostgreSQL queries, and which queries contribute most to it at each moment. Similarly, a stacked chart for total I/O bytes per second or WAL bytes generated can be shown. These charts let you overall system resource usage (as we selected to top consumers of each resource) and the proportionate impact of each query on the system’s resources. If one query’s area suddenly grows to dominate the CPU chart, you know it became a hot spot.
Or, as another example, suppose the executions/sec for a query are steady but its I/O bytes line climbs sharply; that implies each execution is doing more I/O than before (for example, due to table/index bloat).
These charts also help to analyze intra-query dynamics – An increase in one query's resource consumption that coincides with a drop in another query's execution rate suggests resource contention between the two.
The particular benefits of time-series are that they allow to see tendencies, including long-term, and catch transient events (like an unexpected batch job running) that happened in the past.

#### Analyzing Query Aggregates with Summary Tables

While time-series charts show trends and live patterns, engineers often need an aggregate view to answer, “What happened in the last hour (or during a specific time window) in total?” For this, the observability system provides summary tables in Grafana that aggregate query statistics over a selected time range. This functions similarly to an Oracle AWR report but in a more flexible, ad-hoc manner.

When you select a time frame in Grafana, the summary table calculates the delta of each metric for each query over that interval. In other words, it shows how many times each query was called in that period, how much total time it consumed, how many rows it processed, how many buffer or I/O bytes it read/wrote, etc. Each row corresponds to one query (from the “top queries” set), and each column is a metric (calls, total time, CPU time, I/O bytes, etc.), showing the total accumulated values within the interval.

This single combined table replaces what might otherwise be multiple sections of a traditional report. Because the set of queries is pre-filtered to the most important ones (via the multi-dimensional ranking described earlier), the table inherently surfaces the top contributors for any metric. An engineer can sort the table by any column to identify the biggest consumers for that specific resource during the period of interest. 

The data can be easily exported to CSV if deeper offline analysis or sharing is needed. This approach is akin to what the pg_profile tool or a custom workload analysis might do: sample the stats, compute differences, and rank top “heaviest” queries by various metrics ￼. The difference here is that it’s seamlessly integrated into the live monitoring dashboards.


In summary, the Query Observability components of this system help experienced engineers quickly find which SQL statements are the culprits for performance issues, both in real-time and retrospect. By focusing on top queries and providing both trend graphs and aggregated reports, it maps to common investigative steps: first, see the big picture of query load; then drill down into the worst offenders.

## Observing Table and Index Activity

After identifying expensive queries, the next logical question is: what database objects (tables and indexes) are those queries touching, and how are those objects performing? Slow queries often indicate problems like table bloat, missing indexes, or inefficient index usage. Thus, the observability system dedicates views and dashboards to table and index activity, employing a similar “top-N by any metric” strategy to focus on the most significant tables and indexes.

### Table Activity

The table activity view draws from pg_stat_all_tables and pg_statio_all_tables (which include per-table statistics on access and tuples) and joins with pg_class to get metadata like table names and sizes. The goal is to capture tables that are noteworthy in at least one dimension of activity or size. Key metrics and inclusion criteria include:
	•	Table Size (on disk): Very large tables (e.g. top 100 by size) are included. Size matters because large tables can lead to longer seq scans, more memory usage, and prolonged vacuum times ￼. Even if a large table isn’t very active, its sheer size could pose a future performance issue or require partitioning.
	•	Live Tuple Count: Tables with the highest number of live rows. This often correlates with size, but a table could be large in row count but not in disk size if each row is small. High row count might mean heavy insert activity or a large working dataset.
	•	Dead Tuple Count: Tables with the most dead tuples (rows that have been deleted or updated and not yet vacuumed). A high dead tuple count indicates bloat and that autovacuum might be needed or is not keeping up. Such tables might soon suffer from inefficient scans and should be vacuumed. Capturing top tables by dead tuples helps find bloat-prone tables before they become too large.
	•	Write Activity: Tables with the most rows inserted, updated, or deleted. Each of these is considered separately (e.g., top tables by inserts, by updates, by deletes). This highlights different usage patterns – one table might be top for inserts (like an append-only log table), another for updates (a heavily updated configuration table, for instance), and another for deletes (perhaps a table that is cleaned up regularly). High write activity can mean higher bloat and vacuum load, and it might signal the need for index tuning (indexes on high-write tables can slow down those writes).
	•	Read/Scan Activity: Tables with the highest number of sequential scans and those with the highest number of index scans. A sequential scan count increasing means queries are often reading the entire table (which could suggest missing indexes or that the table is small enough that seq scan is preferred). A high index scan count means the table is frequently accessed via indexes (which is good if those are efficient). By capturing both, we ensure we notice tables that are hot in reads either way. If a usually index-accessed table suddenly appears with many sequential scans, that’s a red flag (perhaps the index is not being used due to planner changes or it became inefficient).

The view takes the union of tables that rank in the top N of any of those categories. This results in a “Top Tables” list that includes a mix of different profiles: large but possibly idle tables, small but extremely busy tables, tables at risk of bloat, and so on. Engineers can then focus on this reasonable subset of tables instead of wading through stats for every table.

Why it’s useful: This answers the typical questions “Which tables are growing the largest?”, “Which tables are undergoing heavy writes (and may bloat)?”, and “Which tables are being read most often?”. For example, if a table shows up as top-5 in updates and also has a very high dead-tuple count, it tells you that table might be experiencing bloat due to frequent updates and possibly insufficient vacuum frequency. Another example: a table that is not very large but appears in the top sequential scans list might indicate queries scanning it repeatedly without an index – possibly a candidate for adding an index to improve performance ￼. By having this top-N approach, the monitoring ensures that whether the concern is size, writes, reads, or bloat, the important tables will surface.

### Index Activity

Indexes have their own view (using data from pg_stat_all_indexes, pg_statio_all_indexes, and pg_class) to capture the most critical indexes. Similar to tables, different metrics shine light on different concerns:
	•	Index Size: Very large indexes (top N by size) are captured. Large indexes not only consume disk space but also incur overhead on writes (every insert/update has to maintain the index). An overly large index that isn’t justified by usage might be a candidate to drop. Also, a large index might indicate a table that itself is large or has high cardinality columns indexed.
	•	Index Scan Count: Indexes that are used most frequently (many scans) are included. This highlights which indexes the application relies on heavily – these should be performing well. If a critical index is used constantly, we want to monitor its performance (e.g., see if it’s becoming a bottleneck).
	•	Tuples Read vs. Tuples Fetched: These metrics (idx_tup_read and idx_tup_fetch from pg_stat_all_indexes) indicate how many index entries were scanned vs. how many table rows were actually retrieved as a result. An efficient index usage would have tup_read not far above tup_fetch (meaning most index entries read led to retrieving a tuple). If an index has a very high tup_read but low tup_fetch, it suggests a lot of scanning through the index that doesn’t end up finding many matching rows – possibly an index used for range scans or a low-selectivity index. Such an index might be inefficient: queries using it scan many index pages but retrieve little data, possibly indicating the index isn’t very selective or that a better multi-column index might be needed for that query pattern.
	•	Index I/O: The view also ranks indexes by their buffer I/O – specifically, those incurring the most reads from disk (idx_blks_read) and those with most hits in memory (idx_blks_hit). An index with high disk reads might not be fully cached, so heavy usage could be causing I/O traffic. This could justify increasing cache (shared_buffers) or might suggest the index or table is too large to cache fully. An index with high usage that stays in cache (many hits, few reads) is performing well; one with many reads might be a performance limiter if storage is slow.

Like with tables, any index that is near the top in any of these metrics is included in the “Top Indexes” set. This means the set covers a range: from the largest indexes (which have maintenance overhead), to the most-used indexes, to potentially inefficient indexes.

One especially interesting analysis the system provides is identifying “useless indexes.” This is done via a summary table that looks at the ratio of writes to a table vs. usage of an index. For each index, it can compare how often the index is updated (which happens on every insert/update to the table that affects the index) against how often that index is used in scans. If an index is being updated constantly (due to table writes) but hardly ever read in queries, it imposes cost with little benefit. Such indexes are strong candidates for removal ￼. In the dashboard, a “useless index” table might list indexes where, for example, the number of tuples inserted/updated in the table is 1000x higher than the number of index scans. Engineers frequently look for these cases because dropping an unused index can free up disk and improve write performance with no negative impact on query speeds ￼.

Why it’s useful: The index view addresses questions like “Do we have unused indexes bloating our DB?”, “Which indexes are critical to workload?”, and “Is any index becoming a bottleneck due to size or I/O?”. For instance, a sudden drop in an index’s scan count coupled with a rise in its table’s sequential scans can alert you that the optimizer stopped using that index (maybe statistics changed or the index became less effective). That could directly correlate with a query slowdown. By catching that pattern, you know to investigate why the index usage changed – perhaps auto-analyze didn’t run in time or the index lost utility. On the other hand, identifying an index that is seldom used but has a high write maintenance cost can lead to a quick win by removing it ￼. All these insights tie back to typical DBA tasks: ensure indexes are helping, not hurting.

### Visualizing Table and Index Metrics in Grafana

The collected table and index stats are presented in Grafana in two complementary ways: an inventory table for point-in-time state, and time-series charts for dynamic behavior.
	•	Inventory Summary Table (Point-in-Time Analysis): This is a table panel that lists key metrics for each of the top tables or indexes at the end of the selected time range. Essentially, it provides a snapshot of the current state (or the state at the end of an examined period). For tables, it might display columns such as: current size in bytes, total live tuples, total dead tuples, total inserts/updates/deletes that have occurred (as of now), and total sequential or index scans so far. For indexes: current size, total index scans, total tuples read/fetched, etc. These are gauge values (cumulative counters or absolute values) reflecting the latest known state.
This inventory view is useful for an exploratory overview. For example, you can instantly see which tables are the largest in the database right now, which tables have a lot of dead tuples at the moment (pointing to bloat), or which index has the highest number of scans so far. If you adjust the time range to “last 24 hours” and use the inventory at the end, it essentially shows the state as of now (since these stats mostly accumulate over uptime). It’s akin to a quick status report of the top tables and indexes. An engineer might use it to answer “What are my top 10 biggest tables and how bloated are they currently?” or “At this moment, which indexes have been used the most?”.
	•	Time-Series Charts (Rate of Change for Activity): To observe behavior over time, Grafana also plots time-series for certain table and index metrics. Importantly, these charts often show rates (deltas per second or per minute) rather than raw counters, because we want to see the activity intensity. For example:
	•	A chart of tuples inserted per second for a particular table (or a group of top tables) shows how insert throughput changes over time. If a batch process loads data into a table, you’d see that table’s insert rate spike during the load.
	•	A chart of sequential scans per second vs index scans per second on a table can reveal query plan changes. Normally, you might see a steady rate of index scans on a table. If a drop in index scans is accompanied by a rise in sequential scans, it strongly suggests the queries stopped using the index and are doing full-table scans. This could happen due to outdated statistics or a disabled index, and it usually correlates with performance issues for those queries. Engineers who have chased “sudden slow queries” will recognize this pattern – the monitoring system makes it visually obvious.
	•	For indexes, a chart of index usage over time (scans per second on each top index) can show if an index’s usage is seasonal or tied to certain jobs. If an index is unused for days and suddenly gets a burst of scans, perhaps a specific report or job ran that finally utilized it.
Another very useful chart is the “Useless Indexes” ratio over time. While the summary table gives a ratio for the selected period, a time-series could track, for each index, the moving ratio of writes to reads. If an index consistently has, say, 10,000 writes for each scan over weeks, it’s a strong candidate to drop. Observing this over time also helps ensure you don’t hastily drop an index that is rarely used except end-of-quarter (some indexes might be vital but only used in monthly reports, for instance). By looking at a longer timeline, you can avoid that pitfall.
	•	Hotspot Analysis (Tuple Locks): (This ties in with the lock monitoring, but since it directly relates to table rows, it’s worth mentioning alongside table metrics.) The system includes a specialized live view for tuple-level locking hotspots, described later. In Grafana, this is typically a live table showing which specific rows (identified by keys) are currently most contended. While not exactly a table metric in the pg_stat sense, it’s directly about table data contentions, complementing the table/index activity view with which pieces of data are hot. We will discuss this in detail in the Locks section, but it’s mentioned here because it often sits on the same dashboard as table metrics to give a full picture of table health (from both a throughput and a contention standpoint).

In practice, engineers use the table and index dashboards to pivot their analysis: after seeing a slow query, they identify the table it hits. Then on the table/index dashboard, they can see if that table is unusual (huge size? many dead tuples? lots of sequential scans?) or if the relevant index is perhaps inefficient or heavily hit. For example, if a slow query is doing an index scan on table X, and you see table X has millions of dead tuples and the index itself has a very high idx_tup_read vs fetch (meaning lots of index pages scanned), you might conclude that bloat is causing the index to be less effective, warranting a VACUUM or REINDEX. The monitoring brings these clues to the surface.

## Observing Long-Running Transactions

Long-running transactions are a common culprit in database slowdowns and operational issues. Experienced PostgreSQL engineers know that a transaction kept open for too long can:
	•	Hold locks that block other queries.
	•	Prevent VACUUM from cleaning up old rows (because the transaction’s snapshot might still need them), leading to bloat.
	•	Consume resources over time (e.g., growing undo data or memory).
	•	Risk transaction ID wraparound issues if they remain open and idle.

The observability system tackles this by continuously sampling the state of active transactions (from pg_stat_activity and related views) and focusing on the ones that are long-lived or otherwise interesting. A custom view augments pg_stat_activity with data from other sources like pg_locks and pg_stat_progress_* to provide a comprehensive snapshot of each long transaction.

For each active session (backend) that is not idle or trivial, the view captures:
	•	Identification: The process ID (PID) of the backend, the database name, the user role, and the client/application name or backend type. This tells you who and where – important when you need to reach out to a team or identify the source of a long transaction (e.g., it might show an application name or say “psql” indicating a manual session).
	•	State and Wait Information: The current state (active, idle in transaction, etc.) and the wait event if any (for example, Lock if it’s waiting on a lock, or ClientRead if it’s waiting for a client). If a transaction is idle in transaction (i.e., has an open transaction but currently not doing work), that’s often a problem – it means someone ran BEGIN and hasn’t committed/rollback nor is sending queries, which can hold resources unnecessarily.
	•	Duration Metrics: How long the transaction has been open (xact_duration_ms), and how long the current query has been running (query_duration_ms). These help distinguish a transaction that is doing a single slow operation vs. one that’s perhaps done work and is just sitting open.
	•	Lock and Blocking Info: The count of locks held by this transaction, how many other transactions it is blocking (if any), and how many others are blocking it (if it is waiting on something). This is crucial for diagnosing live lock issues – for instance, you might see a long transaction that is blocking 5 others (meaning it holds a lock they need), or conversely it’s been waiting on a lock held by someone else for a long time.
	•	Transaction “Health” Indicators: Notably, the transaction’s backend_xmin (the oldest XID this backend’s snapshot sees as uncommitted). This is essentially the lower bound of which old rows cannot be vacuumed because this transaction might still need them. A very old backend_xmin is a red flag for vacuum issues, as it indicates this transaction’s age. The view can calculate how far behind the global latest XID this is, sometimes presented as “XID age.” If this number grows too high, it risks transaction ID wraparound. Another metric is the number of subtransactions (nested transactions via savepoints) currently open, which we’ll discuss separately.

All these metrics are stored as time-series. Grafana then provides:
	•	Live Activity Snapshot Table: A table that refreshes every minute to show the current long transactions and all the above details. This is like a real-time “top transactions” view. An operator can glance at it to answer “Do we have any stuck or long transactions right now?” and “Are any causing a blockage?”. Because it’s updated frequently, it’s almost like a live dashboard of pg_stat_activity but focusing only on sessions of interest (e.g., those that have been open > a certain threshold or currently waiting, etc.). This saves you from manually running queries on the database during an incident – the info is already being captured and shown.
	•	Transaction ID Age Chart: Perhaps the most novel visualization, this time-series chart plots transaction IDs over time to visualize long transactions and XID usage. The Y-axis represents the XID space (which is ever-increasing modulo $2^{32}$). The system draws:
	•	A line for the current global XID (as transactions are started, this increases). This typically goes up steadily over time.
	•	Each active transaction is represented as a horizontal line at the Y-value of its XID (or its backend_xmin). The line spans from the transaction’s start time to its end (commit/rollback). So, a short transaction appears as a very short blip or dot (hardly visible), whereas a long-running transaction appears as a long horizontal line.
	•	The vertical gap between a long transaction’s line and the current XID line above it indicates how far “behind” the system that transaction is in terms of XID progression.
This chart essentially visualizes the age of transactions. You can immediately spot if any transaction has been open for an inordinate time: it will be a long line well below the moving head of XIDs. For example, if one horizontal line persists across the last 2 hours on the chart, that means a transaction started 2 hours ago and is still not committed. If that line is far below the top (meaning many newer XIDs have been consumed while it remained open), it’s an indicator that vacuum might be prevented from cleaning tuples deleted since that XID.
By visualizing it, engineers can grasp not just that there is a long transaction, but also how significant it is relative to the system’s overall workload. A long transaction that barely moves the XID (maybe the system isn’t very busy) is less urgent than one where millions of other transactions have happened meanwhile (large gap). This addresses the concern “Are long transactions affecting maintenance or wraparound?” in a very intuitive way. Instead of just a number in age(datfrozenxid), you actually see the timeline.
	•	Subtransaction Monitoring: A subtransaction is created typically when a SAVEPOINT is used (often implicitly by exception blocks in PL/pgSQL or certain ORMs). PostgreSQL caches up to 64 subtransactions per transaction in memory; if you go beyond that, performance can degrade because it has to start writing to disk and managing an overflow list ￼ ￼. Excessive subtransactions can also contribute to lock table bloat and other overhead. The monitoring view includes the current subtransaction count for each backend, and Grafana charts track subtransactions over time for each long transaction. This means if a transaction is repeatedly adding savepoints (for example, a poorly written loop that uses exception handling for flow control), you’ll see its subxact count rising. If it crosses the max_locks_per_transaction threshold (64 by default), PostgreSQL has to do extra work, which often correlates with a wait event SubtransSLRU or SubtransControlLock showing up in pg_stat_activity ￼. The system allows you to catch that: you might see a spike in subtransactions count and simultaneously perhaps an increase in lock waits.
Knowing which transaction is doing lots of subtransactions is key to identifying the offending code. In many cases, this pattern comes from application frameworks that abuse exception savepoints in loops or from legacy stored procedures. Armed with this info, an engineer might say “Ah, this long transaction has 200 subtransactions, no wonder it’s slowing down – we should refactor that code to avoid so many savepoints.” Indeed, industry advice is to avoid exceeding 64 subtransactions per transaction ￼, and this monitoring makes it clear when that happens.

To summarize, the Long Transactions observability addresses the concern “Is there a transaction that’s been running or open for too long, and what impact is it having?”. By providing a live list and historical visualization of transaction durations and ages, it helps prevent or diagnose issues like vacuum stuck behind an old transaction or a user session inadvertently left open. It directly maps to the common scenario: “The database is not vacuuming, what’s holding back xmin?” – you would look at the XID age chart and likely see a culprit line, then check the live table to get details like which user and what query it might be running. Likewise, for “We see many lock timeouts, is something holding locks too long?” – the live table would show if a particular transaction is blocking others, and the wait events (discussed next) would also shed light.

## Observing Locks and Wait Events

Even with fast queries and well-tuned indexes, a database can bottleneck due to contention – when processes wait on locks or other resources. PostgreSQL provides some insight via pg_locks (active locks) and pg_stat_activity (current waits), but those are instantaneous views. They tell you “right now who is waiting for what,” but they don’t easily answer how often and why waits happen over time. To tackle this, the observability system uses the pg_wait_sampling extension, which periodically samples the wait events of backends.

pg_wait_sampling Profile: This extension can record a sampled profile of wait events associated with queries. The custom monitoring view joins pg_wait_sampling_profile with pg_stat_statements and pg_stat_activity to attribute each sampled wait to a specific query and user. Essentially, it aggregates counts of wait events by the combination of:
	•	The SQL query (normalized query id from pg_stat_statements),
	•	The database user,
	•	The type of wait event (and even the specific event).

It filters out client-side waits (like waiting for the client to fetch results) because those typically indicate the application is slow to read data rather than an internal database issue. The focus is on internal waits such as lock contention (Lock: relation, Lock: tuple, etc.), LWLock contentions, buffer I/O, extension events, etc., which represent the database server waiting on something it can control or that affects throughput.

The result is an aggregated view like: “Query Q by user U was observed waiting on event E N times in the sample period.” A high count means that query often had to wait for that event.

Grafana visualizations for wait events include:
	•	Time-Series Charts of Waits: These charts show the rate at which waits are occurring, broken down by event type, query, or user. For example, you could have a chart for lock waits where each line is a particular query (or table) that is causing lock waits. A sudden spike on one of these lines indicates that at that time, that particular query was getting stuck waiting a lot. If you see “Lock: tuple” waits spiking for an “UPDATE orders SET … WHERE id=?” query, it implies a lot of transactions were contending on the same row(s) in the orders table at that moment. Similarly, an increase in “IO: ClientWrite” might indicate slow network or client consumption, but since those are filtered out, more interesting would be something like “IO: DataFileRead” spiking, meaning queries are waiting on disk reads (possibly hitting slow I/O or cache misses).
By plotting these over time, the system helps correlate wait spikes with other events. For instance, you might notice that every day at 9AM, there’s a spike in “Lock: relation” waits – perhaps a batch job that does a bulk load is conflicting with an analytical query. The time-series view makes these patterns visible where a one-time sample might miss them. Engineers often want to know not just “is there contention?” but “when and how often is contention happening, and who is involved?” This provides that temporal dimension.
	•	Summary Table of Wait Events: For deeper post-mortem analysis, a Grafana table can aggregate total wait event counts over a chosen interval, similar to the query summary table but for waits. It might list entries like: “User X / Query Y / Wait Event Z – 12345 samples”. By sorting this table, you can answer questions such as:
	•	“What was the biggest source of wait time during the maintenance window last night?” Maybe it shows that most waits were “Lock: tuple” on a particular update query, confirming contention as the top issue.
	•	“Which query is causing the most locking?” – Sort by lock wait count and see which query rises to the top.
	•	“Are we CPU bound or I/O bound?” – If you have wait events sampled for CPU (when pg_wait_sampling is configured to sample CPU sleeps as waits), you might see CPU as a wait event versus I/O or lock events. (However, typically CPU usage is the absence of waits – a non-wait – so this is more about non-CPU waits.)

In essence, this part of the observability system connects performance problems to contention factors. When users complain “the database is slow at X time”, it’s often due to waiting (either on locks or on disk). Traditional monitoring might show high lock count or high CPU, but this goes a step further to pinpoint which query was waiting and on what. It’s akin to having a lightweight profiler running on the database, attributing wait time to code paths (queries).

Drilling Down into Tuple-Level Locking

Lock contention on tables or high-level locks is one thing, but a particularly challenging scenario is row-level contention. In PostgreSQL, a common wait event is Lock: tuple which means a transaction is waiting on a row-level lock held by another transaction (typically from an UPDATE or SELECT … FOR UPDATE on the same row). The wait event tells us it’s a tuple-level lock, and pg_stat_activity might even tell which relation (table) is involved via relation column, but it does not tell which row is hot. When investigating severe concurrency issues, engineers often need to know if there is a single “hot row” (like one customer account that many transactions are contending on).

To address this, the observability system includes a specialized view (referred to as ak_tuple_locks) that zeroes in on tuple-level locks in the system. Here’s how it works:
	•	It queries pg_locks filtering for locks of type tuple that are in waiting state or exclusive mode. In pg_locks, a tuple lock is identified by a tuple’s physical identifier (ctid: block number and tuple index within block) along with the relation (table OID).
	•	The view joins this with system catalogs (pg_class, etc.) to get the table name (and schema) that the tuple belongs to. It also can join with pg_stat_activity to get which transaction is holding vs waiting on the lock, enabling counting of how many waiters.
	•	It then groups by that tuple identifier (table + ctid) to count how many backends are waiting for the same tuple. It essentially produces a list of the most contended tuple IDs at that sample moment, including possibly the tuple’s coordinates and how many are waiting on it.

Recognizing that a tuple ID like (block 8675, tuple 309) is not user-friendly, the view goes further for certain frequently locked tables: it knows how to reverse-map that physical tuple to a business key by doing a lookup on the table itself (this is done carefully, often via a CASE statement limited to known hot-spot tables, to avoid heavy queries for every lock). For example, if the table is accounts and the tuple with ctid (8675, 309) corresponds to account_num = 'CUST12345', the view will output that key. This way, the operators immediately see which business entity is hot, rather than an opaque tuple address.

In Grafana, this information is shown as:
	•	Live “Hot Tuples” Table: A table visualization listing currently contended rows, refreshed at the same interval (e.g., each minute). It might have columns like: Table name, Business Key (if resolved), Number of waiters (lock count), and perhaps the XID of the locking transaction or age of that lock. The table is sorted by number of waiters, so the most contended row is at top. If there are, say, 15 sessions all waiting on the same row in accounts, that might show up as: accounts – account_num CUST12345 – lock_waiters=15. This immediately flags a hot spot. An engineer seeing this can deduce that maybe a single customer’s account record is extremely contentious (maybe an application design issue causing many concurrent updates to the same account).
	•	Historical Lock Contention Chart: By recording the number of waiters on specific keys over time, you could also plot a graph for a particular entity. For example, track “lock_waiters for account CUST12345” as a metric. Over a day, you might see repeated spikes at certain hours, indicating that account tends to be a hot spot at those times (maybe that customer has a lot of concurrent activity daily). This can feed into design considerations (e.g., maybe that account is a proxy for a bigger entity that should be sharded).

This kind of tuple-level insight is typically very hard to get without custom queries, and yet it’s extremely valuable for troubleshooting blockers and deadlocks. If a critical process is hung, you might find that it’s waiting on a lock, and this view tells you it’s waiting on row X of table Y. With the business key in hand, you can even communicate to application teams: “All the contention is on account CUST12345, perhaps there’s a pathological case in code for that account.” It bridges the gap between low-level database wait info and high-level business context.

Example: Suppose you have a table of inventory and a particular product ID keeps getting its row locked because multiple transactions try to update its quantity concurrently. The dashboard’s hot tuple table will show that product ID as a hot spot when it occurs. If this is a frequent problem, you now have evidence to perhaps implement a queuing mechanism or re-architect how inventory updates are done for that product to avoid the pile-up.

In summary, the locks and waits monitoring components give a full picture of database concurrency: from aggregate wait event profiling down to pinpointing exact rows causing contention. This maps to the troubleshooting questions around “What is the database waiting on?” and “Who is blocking whom?”. With these tools, an experienced engineer can quickly identify if the bottleneck is due to lock contention and whether it’s broad (many different locks) or narrow (one specific lock causing a big jam).

## Observing VACUUM and Maintenance

Autovacuum and analyze processes are the unsung heroes of PostgreSQL maintenance, and when they lag behind, performance can degrade (due to bloat) or even the database can face transaction ID wraparound risks. Therefore, a critical part of observability is monitoring vacuum activity and table bloat indicators. The system includes a custom view that computes, for each table, how close it is to needing a vacuum or an analyze, based on PostgreSQL’s autovacuum settings and the statistics in pg_stat_all_tables.

Key data points in this view include:
	•	Time since last vacuum/analyze: How long (in seconds or days) it has been since each table was last vacuumed or analyzed. A steadily increasing time might be okay if the table is static, but if a table is very active and yet not vacuumed for a long time, that’s a concern.
	•	Dead Tuple Vacuum Ratio: The fraction of autovacuum threshold that the current dead tuples count represents. For example, if autovacuum is configured to trigger at threshold + scale_factor * table_size dead tuples, the view calculates dead_tuples / (autovac_threshold) = some ratio. When this ratio reaches 1.0 (100%), it means the table has enough dead tuples to trigger an autovacuum. If it’s >1.0, it’s beyond the threshold — ideally autovacuum should already be running or have run.
	•	Modification Analyze Ratio: Similarly, for analyze (which triggers on a threshold of modified tuples), the ratio of inserted+updated+deleted tuples since last analyze to the analyze threshold. If this crosses 1.0, the table is due for an auto-analyze (to update optimizer statistics).
	•	Wraparound Age (Freeze) Ratio: The system also looks at each table’s relfrozenxid (the oldest XID that has not been frozen in that table). There is a known safety limit (by default around 2 billion transactions old) at which PostgreSQL forces an aggressive autovacuum to prevent wraparound. The view expresses each table’s current XID age as a percentage of autovacuum_freeze_max_age. If this approaches 100%, it’s a serious warning sign.

The Grafana dashboards present this information in several ways to help engineers anticipate and diagnose maintenance issues:
	•	Vacuum/Analyze Thresholds Time-Series: A chart plots the autovacuum and analyze trigger ratios for tables over time. Each table can be a line that starts at 0 after a vacuum and gradually rises as dead tuples accumulate. When autovacuum runs and cleans the table, its dead tuple count drops and the line resets near 0. By watching these lines:
	•	You can tell which tables are accumulating dead tuples fastest (steeper slope).
	•	See approximately when they reach 100% (which is when autovac should kick in). If a table’s line hits 100% frequently, it means autovacuum is running often for it. If a line goes well beyond 100%, that indicates autovacuum might not be catching it in time.
	•	This helps answer “Are our autovacuum settings keeping up?” If many tables hover just under 100% or occasionally exceed it, you might be at the edge of what the autovac worker threads can handle. If lines are constantly way below 100%, maybe autovacuum is too aggressive (not usually a big problem except wasted effort).
For example, an engineer might notice that a particular table’s dead tuple ratio hits ~120% before dropping, meaning autovacuum always comes a bit late for that table. Perhaps raising autovacuum_vacuum_scale_factor or adding more autovac workers would help.
	•	Maintenance History Timeline: This is a visual timeline (Gantt chart style) of vacuum and analyze events on each table. Each time autovacuum or manual VACUUM runs on a table, it’s shown as a bar on that table’s row, spanning from start to finish time of the operation. Different colors might distinguish VACUUM, VACUUM (to prevent wraparound), and ANALYZE operations. This answers questions like:
	•	“When did autovacuum last run on this table and how long did it take?”
	•	“Are multiple autovacuums running concurrently on different tables?” – you might see overlapping bars on different tables.
	•	“During the incident last night, was an autovacuum running?” – maybe you see that at 3 AM, a large autovacuum on orders table was happening which could have slowed things.
By correlating this timeline with query performance graphs, one can spot if autovacuum activity impacted query timings. For example, if response times spiked at the same time a vacuum was running on a large table, it could indicate I/O contention from vacuum.
	•	Diagnosing Autovacuum Issues: The combination of the above charts is particularly powerful for diagnosing scenarios where vacuum isn’t keeping up:
	•	Ineffective Vacuum: If the dead_tuple_vacuum_ratio for a table keeps rising above 100% even though the timeline shows autovacuum ran, this suggests vacuum ran but couldn’t remove the dead tuples. The usual cause is a long-running transaction or replication slot holding back xmin, preventing vacuum from reclaiming old rows. The dashboard would show, for instance, table accounts dead tuple ratio at 150% and climbing, while an autovacuum bar is active on accounts. In that case, you switch to the Transaction Age chart and likely see a long horizontal line (a transaction) with a very old XID. That’s your culprit. This joined-up monitoring saves time – it tells you what is wrong (vacuum can’t clean table X) and why (transaction Y has been open for N hours holding things up) without manual guesswork. As a best practice: if you see this, the next step is to terminate the long transaction or investigate why it’s open.
	•	Wraparound Prevention: The system especially highlights when the freeze age ratio for any table exceeds 100%. In the timeline, a “wraparound autovacuum” (the emergency one that ignores cost delays) might appear. If such a process is running and yet the age still climbs, that’s critical – it means even the emergency vacuum cannot finish. The likely reason is the same: an old transaction prevents it from marking tuples as frozen. The monitoring will make this very obvious, flashing essentially a warning that “Table X is in danger of wraparound!” Perhaps even an alert can be set on that metric. Engineers know that if wraparound autovacuum can’t complete, the database will eventually refuse new transactions – a catastrophic situation – so catching it early is vital.
The dashboards thus map directly to the question “Is autovacuum working properly? If not, what tables are trouble and why?”.
	•	Checkpoint Monitoring: Alongside vacuum, checkpoints are another operational aspect to monitor. A checkpoint occurs either at a timed interval (controlled by checkpoint_timeout, typically 5 minutes) or when the WAL write volume hits a limit (max_wal_size triggers an early checkpoint). Checkpoints flush dirty buffers to disk and can cause I/O bursts; too frequent checkpoints can degrade performance, but too infrequent means longer recovery if crash. The Grafana timeline for checkpoints shows each checkpoint occurrence, marking whether it was timed or requested (early). If you see frequent requested checkpoints (say the chart shows a checkpoint requested every 2 minutes, not aligning with the 5-minute timeout), that indicates the system is generating WAL faster than max_wal_size can accommodate, thus triggering early checkpoints. An experienced engineer would recognize this as a tuning issue: perhaps increase max_wal_size to avoid so many early checkpoints ￼, or if that’s not possible, it’s at least known that the I/O load from checkpoints might be affecting performance. Conversely, if only timed checkpoints occur and far apart, it’s healthy (assuming WAL volume is under control). This visualization helps answer “Are we checkpointing too often?” and can guide adjustments to parameters or application behavior (like bulk loads could be throttled to reduce WAL bursts).

Bringing it together, the VACUUM and maintenance observability caters to the concerns: “Do we have bloat building up? Are our vacuums and analyzes running in time? Is any table at risk of wraparound? Are periodic processes like checkpoints impacting performance?” By monitoring thresholds and history, it turns what is usually discovered when it’s too late (e.g. “Oh no, the database froze due to wraparound!”) into something predictable and avoidable.

For example, one might discover through these tools that a certain large table always approaches the autovacuum threshold shortly after a bulk load each day, causing an autovacuum during peak traffic – seeing this pattern, they might decide to schedule a manual vacuum at a safer time or adjust autovac settings for that table. Or, one might spot that the cache hit ratio (from standard stats) dropped whenever an autoanalyze hadn’t run for a long time on a table, indicating stale statistics led to bad plans – prompting a more frequent analyze schedule.

Observing Standard PostgreSQL Statistics

Beyond the deep dives into queries, locks, and vacuum, the observability system also tracks the “standard” PostgreSQL performance metrics available in the pg_stat_* views. These are essential for a general health check and often serve as early warning signals for issues. Some of the key statistics and how they’re visualized include:
	•	Transactional Throughput: Metrics like transactions committed and rolled back per second (from pg_stat_database). A healthy system will have a steady rate of commits. Spikes or drops in commit rate can indicate changes in workload. A sudden drop to zero might mean the database or application is down. The rate of rollbacks is also telling – a high rollback rate could indicate a lot of errors or retries in the application. Typically, one expects rollbacks to be rare; if they spike, something might be wrong at the application layer.
	•	Cache Hit Ratio: This is derived from comparing buffer hits vs reads (pg_stat_database.blks_hit vs blks_read). Grafana can plot the ratio or the underlying values. A high cache hit percentage (close to 99% or more) is usually desired. If you see the ratio dipping, or the volume of disk reads rising relative to hits, it suggests either the working set doesn’t fit in RAM or a sequential scan is pulling in lots of new data. It’s a prompt to investigate queries causing cache evictions or consider increasing shared_buffers. Keep in mind that PostgreSQL’s reported cache hit is at the PostgreSQL buffer level and doesn’t count OS cache hits ￼. However, a low value here generally correlates with more direct I/O. The monitoring charts might show both the absolute bytes read from disk vs bytes served from cache per second ￼ ￼, giving a sense of how much the database is hitting disk. Ideally, most of the time series will be in “hit bytes” with minimal “read bytes” ￼ – deviations mean queries that aren’t cached.
	•	Temporary File I/O: The amount of data written to temporary files (pg_stat_database.temp_bytes) per second. This indicates how much query work is spilling to disk (for sorting, hashing, etc., when it can’t fit in work_mem). If temp file usage is high, queries may benefit from more memory or query optimization to avoid large on-disk sorts. Spikes in temp_bytes often correlate with heavy reporting queries or inefficient operations. Monitoring this helps answer “Are we doing a lot of disk-sorting or temp file usage lately?” which if yes, could degrade performance and is a target for tuning.
	•	WAL Generation Rate: The volume of WAL (write-ahead log) bytes generated per second (pg_stat_database.wal_bytes). This reflects the write workload and is tied to both the amount of data being modified and how efficiently (bulk operations generate WAL differently than trickling inserts). If WAL bytes/sec suddenly doubles, it means a heavier write transaction load (or something like a full-table vacuum or a bulk data load). It’s a useful measure when thinking about replication lag or disk bandwidth – e.g., heavy WAL generation can bottleneck a standby or saturate disks.
	•	Checkpoint Activity: As described, metrics such as checkpoints_timed vs checkpoints_req from pg_stat_bgwriter show how many checkpoints were scheduled vs requested. Grafana might plot cumulative or rates of checkpoints to easily see if requested (unscheduled) checkpoints are happening frequently. Ideally, most checkpoints should be timed (planned) rather than requested (unplanned due to WAL filling) ￼. Too many requested indicates the need for tuning.
	•	Connection/Backend Counts: The number of active connections can be charted – possibly broken down by state (pg_stat_activity.state) if the monitoring view pre-aggregates it. For example, count of connections active, idle, idle in transaction, waiting, etc. If “idle in transaction” count is more than a handful, that’s a potential problem (those are sessions holding open transactions doing nothing). A rising number of active connections might mean connection pooling is not working or an unexpected load burst. Also, tracking max connections usage is important to avoid hitting the limit.
	•	Lock and Deadlock Counters: While detailed wait sampling is provided, a simpler metric like deadlocks count from pg_stat_database shows if deadlocks are happening (even if rare). A time-series of deadlocks can alert if a new code deployment suddenly started causing deadlock occurrences. Similarly, one can monitor the total count of locks held or the lock wait count (if available) – though these are more snapshot-y, they can be polled.
	•	Background Writer and Checkpointer Stats: For completeness, metrics like buffer writes by background writer, number of buffers written in checkpoints, etc., can be tracked. These help in tuning bgwriter or seeing if checkpoints are writing a lot of buffers (which ties to checkpoint frequency as well).

All these standard metrics are typically visualized as simple graphs (lines or bars). For instance, a panel might show “Transactions per second (commits vs rollbacks)” where you want to see mostly commits. Another might show “Cache Hit % vs time” to ensure it stays high. These are the kind of metrics any seasoned PostgreSQL DBA would have on a dashboard to watch overall health.

Scenario: Suppose an application is slow, but query-specific graphs don’t reveal an obvious culprit. Looking at standard stats, you notice that at the slow time, the transactions per second dropped while the rollback rate spiked – that indicates many transactions were failing or retrying. Simultaneously, you see an increase in deadlocks count. This tells a story: there was likely a surge in contention leading to deadlocks, many transactions had to rollback and retry, thus lowering completed TPS. This pushes you to investigate application logs or recent changes in how transactions interact (maybe a new code path that causes deadlocks). Without these broad stats, one might have focused only on slow queries and missed the bigger picture of why throughput was low.

In sum, the standard statistics act as a safety net and context provider. They ensure that while deep in the analysis of specific queries or locks, you don’t miss an obvious system-wide trend (like overall I/O increasing, or commit rate dropping). They map to the routine checks an engineer does: “Is the database generally healthy? High cache hit? No unexpected surges in I/O or temp files? No accumulation of idle connections?” All that can be passively monitored here.

## Grafana Techniques for Deeper Analysis

Having a rich dataset is one thing, but effectively analyzing it in Grafana is another. The system uses a couple of Grafana features to make analysis easier, especially in scenarios with lots of data series or the need to drill down into a subset.

Using Logarithmic Scales for Visibility

When viewing metrics that are broken down by high-cardinality labels (for example, resource usage by each query, or wait counts by each event type), there’s often a huge skew: a few items dominate the metric, and many others are relatively small. On a normal linear scale, the dominant series (say, one query that uses 80% of CPU) will tower over the others, making the smaller ones appear flat near zero. This is problematic if you want to examine patterns of the smaller ones – you literally can’t see their variance because it’s all compressed at the bottom of the chart.

To address this, the Grafana dashboards make use of the logarithmic scale option on relevant charts. By switching the Y-axis to a log scale, changes in smaller values become visible even when larger values exist. The log scale essentially “compresses” the large values and “expands” the small values, in terms of visual separation.

For example, consider a chart of “CPU time by query” with 10 queries. Query A consumes 10,000 ms/s of CPU (very heavy), and Query B, C, D consume maybe between 100 and 1000 ms/s, and the rest below 100. On a linear scale 0 to 10000, Query A’s area dominates; queries that are at 100 ms look almost like 0. On a log scale, Query A’s 10000 is perhaps a bit above the others but you can now clearly see that Query B went from 100 to 500 (a 5× increase) as a noticeable jump. Patterns like “Query C gradually increasing usage” become apparent even if Query A is always the biggest.

This is extremely useful for spotting relative changes. Engineers often care not only about the top talker but also if some previously negligible query is ramping up resource usage (maybe a bug causing it to be called more and more). The log scale reveals trends in those smaller series which would be hidden otherwise.

Another use is when looking at wait events or errors that usually are zero but occasionally spike. On linear scale, a spike from 0 to 5 is visible, but if at the same time another metric goes to 1000, the smaller spike is lost. Log scale will show both spikes in their own magnitude.

The key point is that using a log scale de-emphasizes the absolute differences and highlights the multiplicative or percentage changes. The monitoring guide encourages using this perspective when one or two elements are so large that they drown out the rest – it’s a way of saying “ignore the big guy for a moment, do any of the others show interesting movement?”. Of course, one must interpret carefully (log scale can make small differences look bigger), but for seasoned engineers it’s a powerful way to inspect the data.

Creating Focused Views with Dashboard Variables

When debugging a complex performance issue, it’s often useful to narrow the scope to a particular query, user, or database object and see everything about it. Grafana’s dashboard variables feature is used to enable this kind of pivoting without leaving the interface or writing custom queries each time.

The observability setup provides context-specific dashboards or panels that are driven by selected variables. For instance, there might be a dropdown for “User” which filters all panels on the dashboard to only show data for that user. Similarly, a “Query ID” or query fingerprint selector could exist, or a “Table Name” selector.
	•	If an engineer selects a particular user from a dropdown, the dashboard can then show:
	•	All the queries run by that user (time series of their query execution rates, their top queries summary, etc.).
	•	The wait events attributable to that user’s queries only.
	•	The connections or sessions for that user.
	•	Essentially a profile of what that user’s activity looks like.
This could answer, for example, “What is this reporting user doing that is causing load?” – flip to that user, and you see they are running a heavy aggregation query every hour that uses lots of CPU and I/O.
	•	If a query pattern is selected (say by an ID or a part of the normalized query text), the dashboard might reconfigure to show:
	•	That query’s execution count over time, its average latency over time.
	•	Its resource usage (CPU, I/O) over time.
	•	How many rows it returns over time (to catch if, for example, the result set size has grown and maybe that’s slowing it).
	•	Any wait events associated specifically with that query.
	•	Perhaps which tables that query interacts with (if the metric schema allows linking query to tables, sometimes by parsing the query or via known query-group).
This focused view is like zooming in on one query as if the rest of the system doesn’t exist. It’s very handy when you’ve identified a suspect query and want to gather all evidence around it: Is it always slow or only during certain hours? Does it cause I/O waits? Did it get slower after a certain time (maybe after stats changed)? Without such a dashboard, an engineer would manually filter each graph – here it’s one selection to pivot.
	•	For a specific table, a similar concept could apply:
	•	Show only activity on that table: e.g., how many inserts/updates on that table over time, how many sequential vs index scans on it.
	•	Show the top queries that involve that table (if the system tracks query → tables mapping or if the query text can be filtered by table name).
	•	Show any lock events or waits involving that table.
	•	See the table’s bloat metrics, vacuum history just for that table, etc.
This answers questions like “Why is table X always a source of problems?” – one glance and you see that it has a high write rate and regularly hits autovacuum thresholds and often has lock waits on it. That builds a case that maybe table X should be partitioned or indexed differently.

The use of dashboard variables essentially turns the observability platform into an interactive investigative tool. It’s not just showing overall metrics, but allows drilling down into perspectives – user-centric, query-centric, table-centric views. This is especially important when a large system has many users and workloads; it’s easy to get lost in the aggregate. The variables let you declutter the view and focus on a slice relevant to the issue at hand.

From an experienced engineer’s standpoint, this feature maps to the way one often troubleshoots: by isolating components. If a particular microservice or user is blamed for load, you’d filter all your analysis to just that in order to confirm. Grafana variables make this one-click.

⸻

Conclusion: The PostgreSQL observability framework described above is tailored to connect the internal state of PostgreSQL to the practical performance concerns engineers have. It combines low-level statistics with insightful aggregation and visualization, ensuring that no matter what angle an issue comes from – slow query, stuck transaction, bloat, lock contention, or system configuration – you have a window into it. By being didactic in nature, each dashboard essentially guides the engineer to ask the right questions: “Is it the query or the table?”, “CPU or I/O?”, “Lock or long transaction?”, “Autovacuum or something else?”. And for each question, it provides the data to answer it, usually with just a sort or a filter in a Grafana panel.

The emphasis on top-N filtering for queries, tables, and indexes guarantees that the monitoring remains performant and focused on what matters, even as the system scales to billions of events. At the same time, the ability to drill down with variables and log scales means you can peel back layer by layer – from overview to detail – without losing context.

For an experienced PostgreSQL engineer, this kind of observability feels like an extended version of the familiar tools (pg_stat_statements, pg_stat_activity, etc.), supercharged to handle high volume and presented in a way that aligns with the investigative thought process. It turns what could be a manual, time-consuming correlational exercise into a mostly visual, interactive experience. The result is faster root cause analysis and more proactive performance tuning, which is exactly what’s needed in a demanding environment like a Tier-1 telecom platform handling hundreds of millions of transactions.

Overall, this document has detailed how each piece of the system maps to typical performance analysis tasks:
	•	Identifying heavy queries and their resource usage trends.
	•	Relating query load to the underlying tables and indexes, spotting bloat or inefficiencies.
	•	Keeping an eye on long transactions and preventing them from wreaking havoc.
	•	Profiling wait events to uncover lock contention and other bottlenecks.
	•	Zooming into specific “hot rows” when contention is high.
	•	Monitoring vacuum progress so bloat and wraparound don’t catch you off guard.
	•	Watching general stats to ensure overall health (throughput, caching, errors).
	•	Using Grafana effectively to explore the data from different angles.

Armed with this observability framework, engineers can move from reactive firefighting to a more observant, data-driven performance culture, where issues are caught early or even predicted, and troubleshooting is significantly accelerated by having all the relevant information at their fingertips.

Sources:
	•	Postgres Professional, PostgreSQL performance issue investigation with pg_profile – outlines the approach of sampling statistics (queries, tables, indexes) to identify “top” resource consumers ￼ ￼, emphasizing the ability to catch frequent fast queries and hot objects without heavy logging ￼.
	•	Laurenz Albe, Subtransactions and performance in PostgreSQL – explains performance issues that occur when transactions exceed 64 subtransactions, leading to pg_subtrans access and SubtransSLRU waits ￼ ￼. Recommends not using too many subtransactions per transaction ￼, which justifies monitoring subtransaction counts.
	•	Datadog, Key metrics for PostgreSQL monitoring – discusses important metrics like sequential vs index scans ￼ and underutilized indexes that might be dropped to save overhead ￼. It also highlights how table/index size growth can indicate bloat or maintenance issues ￼.
